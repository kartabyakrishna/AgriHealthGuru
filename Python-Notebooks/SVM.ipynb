{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d2720d",
   "metadata": {},
   "source": [
    "Using multiple SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae5038e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Filepath looks like a hdf5 file but h5py is not available. filepath=AlexNetModel.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load your pre-trained model (replace 'model.h5' with the path to your HDF5 model)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlexNetModel.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(image_paths):\n\u001b[0;32m     16\u001b[0m     features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\saving\\save.py:236\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilepath looks like a hdf5 file but h5py is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hdf5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    242\u001b[0m             tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(filepath_str, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    243\u001b[0m             custom_objects,\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile):\n",
      "\u001b[1;31mImportError\u001b[0m: Filepath looks like a hdf5 file but h5py is not available. filepath=AlexNetModel.hdf5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your pre-trained model (replace 'model.h5' with the path to your HDF5 model)\n",
    "model = tf.keras.models.load_model(\"AlexNetModel.hdf5\")\n",
    "\n",
    "def extract_features(image_paths):\n",
    "    features = []\n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, (224, 224))  # Adjust input size as per the model's requirements\n",
    "        img = preprocess_input(img)  # Preprocess image data as required by InceptionV3\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        feature_vector = model.predict(img)\n",
    "        features.append(feature_vector)\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Replace 'train_data' and 'valid_data' with the paths to your training and validation data directories\n",
    "train_data_dir = 'data/train/'\n",
    "valid_data_dir = 'data/valid/'\n",
    "\n",
    "# List all image file paths in the training and validation directories\n",
    "train_image_paths = [os.path.join(train_data_dir, filename) for filename in os.listdir(train_data_dir)]\n",
    "valid_image_paths = [os.path.join(valid_data_dir, filename) for filename in os.listdir(valid_data_dir)]\n",
    "\n",
    "# Extract feature vectors from the images using the pre-trained model\n",
    "train_features = extract_features(train_image_paths)\n",
    "valid_features = extract_features(valid_image_paths)\n",
    "\n",
    "# Load labels for the training and validation data\n",
    "# Assuming you have a list of labels for your images\n",
    "labels = ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___healthy', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Potato___Late_blight', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___healthy', 'Strawberry___Leaf_scorch', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___healthy', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_mosaic_virus', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']  # List of labels for both training and validation images\n",
    "\n",
    "# Train separate SVMs for each class using a one-vs-all strategy\n",
    "svm_models = {}\n",
    "for class_label in set(labels):\n",
    "    # Create a binary label for the current class\n",
    "    y_train_binary = np.array(labels) == class_label\n",
    "    \n",
    "    # Create an SVM classifier and train it\n",
    "    svm = SVC(probability=True)\n",
    "    svm.fit(train_features, y_train_binary)\n",
    "    \n",
    "    # Store the trained SVM model\n",
    "    svm_models[class_label] = svm\n",
    "\n",
    "# Classify a new image\n",
    "def classify_image(image_path):\n",
    "    feature_vector = extract_features([image_path])\n",
    "    class_probabilities = {}\n",
    "    for class_label, svm in svm_models.items():\n",
    "        probability = svm.predict_proba(feature_vector)[0][1]\n",
    "        class_probabilities[class_label] = probability\n",
    "    predicted_class = max(class_probabilities, key=class_probabilities.get)\n",
    "    return predicted_class\n",
    "\n",
    "# Test the classification on a sample image (replace with your own image path)\n",
    "sample_image_path = 'path/to/your/sample/image.jpg'\n",
    "predicted_class = classify_image(sample_image_path)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90142a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
